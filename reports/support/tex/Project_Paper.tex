
%% bare_jrnl.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/

% formatting instructions
% https://ras.papercept.net/conferences/support/files/IEEEtran_HOWTO.pdf

% from here
%https://journals.ieeeauthorcenter.ieee.org/create-your-ieee-journal-article/authoring-tools-and-templates/ieee-article-templates/templates-for-transactions/

\documentclass[journal]{IEEEtran}

\usepackage{cite}
\usepackage{amsmath}
\usepackage{url}
\usepackage{multirow}

% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
\graphicspath{{../images/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi

\begin{document}

\title{Big Data Solution for\\ Stock Prices and Tweet Collections}

\author{Paul~Adams, paula@smu.edu,
        Rikel~Djoko, rdjoko@smu.edu,
        and Stuart Miller, stuart@smu.edu}% <-this % stops a space

% The paper headers
\markboth{MSDS7330}{MSDS7330}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.

% make the title area
\maketitle

% This should be about 250 words
\begin{abstract}
\textbf{Purpose}-This dissertation aims to discover an optimal way to manage and process financial markets data to drive multiple algorithmic models for securities trading using the Hadoop ecosystem. This includes an applied, in-depth analysis of parallel processing using the Elastic MapReduce package on Amazon Web Services and the application of data warehousing using Apache Hive. Additionally, Apache NiFi is used to direct workflow automation scripting to migrate data into the warehouse. Finally, a complete assessment of the combinations of levels of the various Hadoop ecosystem applications used in this study will be provided in the context of statistical analysis for inference.
\\
\textbf{Design, Methodology, and Approach}-One pertinent, underlying hypothesis within this study is to prove that there are differences in processing speeds between S3 and HDFS using normalized and optimized schemas across multiple MapReduce configurations. This analysis is performed using 100 samples of the same volume of data in a repeated measures analysis using a Hotelling-T statistic. The two highest-performaning configurations of S3 and HDFS are then assessed. (We need to get this information and report it) for the next section.
\\
\textbf{Findings}-Applying S3 with an optimized schema using 10 reduces, map memory allocation of 2,048 megabytes and a reduce memory allocation of 4,096 megabytes is optimal for a more expensive S3 approach. Using HDFS from local storage with a configuration of 20 reduces, map memory allocation of 8,096 megabytes and reduce memory allocation of 10,020 megabytes is ideal for batch-level migrations and querying for large-volume processing. Across n repeated measures, using a one-tailed alpha, our p-value is significant at Pr < x.xxxx (confidence interval (x1, x2)), indicating local storage from HDFS outperforms S3 when using the selected configurations. However, local data storage capacity does not scale well for HDFS compared to cloud-based S3.
\end{abstract}


% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}
% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps.
% 
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
% 
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by the IEEE):
% \IEEEPARstart{A}{}demo file is ....
% 
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
% 
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.
\IEEEPARstart{D}{ata} ``starter file'' in the twenty-first century is exploding in volumes at an exponential rate; every additional source of data that can act as a medium for data communication can obtain useful information, which, with modern technology, can be structured and stored, accessible to any who have the skills and need to make use of it. This information is increasingly profitable. As such, the scalability of storing and accessing this data must increase with it.
\\
\subsection{Apache Hadoop Ecosystem}
Motivated through the opportunity within ``big data'' and parallel computing, which enables massive amounts of data to be rapidly accessed for complex analysis and distributed across a scalable, cost-effective distribution of servers, we aligned our project with a modern database application, Hadoop, which provides a data lake ``ecosystem'' supporting both task- and data-parallelism across the many software applications within the Apache suite in addition to software that can be managed and accessed within a network of servers, called a cluster.
\\
\subsection{Hadoop MapReduce}
The cluster of server nodes enables users to read data from the same source, simultaneously, as the data at that source is partitioned and processed across multiple servers – a master and at least one slave – through leveraging a processing framework called MapReduce to assign nodes for ``mapping'' and ``reducing'' by applying various configurations, such as related to memory allocation to and volumes of mappers and reducers. As data is mapped, it is reprocessed into a derivative data set, split into tuples that are then processed across the cluster of server nodes, in parallel, and reassembled in the reduce process from which it is delivered to the end-user, whether it be Enterprise Resource Planning (ERP) or a personal user running a SQL “SELECT” statement.
\\
\subsection{Application Integration and Data Ingestion}
This ability of Hadoop is very important to our problem, which requires rapid storing and accessing of data. Our data, which is stock market data obtained via API from a market data vendor every 15 minutes (primarily New York Stock Exchange and NASDAQ data) in addition to Twitter feeds updated every 5 minutes requires the ability to both store and enable applications to both connect to the data source as well as operate within it. R will be used via ODBC to access and build proof-of-concept models using the data. Once developed, Python will be deployed within the data lake to process and derive data from machine learning decisions.
\\
\subsection{Big Data System Implementation}
In order to do some things with this stuff, we need a place to put it. Not just any place, but a special place that scales well, is quick to give and slow to take, and appreciates large volumes of data and potentially large volumes of simultaneous users. Therefore, we must usher in a new era of discussion; the dawning of the dueling between HDFS and S3. To make the battle interesting, we will root HDFS to local storage and S3 to the ever-scalable cloud. Winner takes all, battle royale. HDFS, meet local personal computer. S3, meet the Amazon Web Services' Elastic MapReduce package.
\\
Furthermore, we need a data warehouse, so naturally, we opted for Apache Hive since it does a pretty good job with SQL-based HQL being easily communicated and stored within various applications and operated by traditional SQL users. This is all brought to you in part by Cloudera Hue. Cloudera Hue, thank you - without you we wouldn't actually be able to see any SQL at all. Not only are you a great face, but you're a great interface as well.
\\
\subsection{Database Schema and Selection}
\textit{Flustered was the frivolous foe who quantified the schema not yet, ho.} Finally, close the introduction out with some more explaining about normalized vs. optimized data, why that matters, and how we'll analyze it.
\\
\subsection{Performance Metrics and Evaluation}
Performance is based on speed taken to process our 3 gigabytes of data - once processed into their respective storage systems - into the Hive data warehouse, which includes table creation and loading. We will use a repeated measures analysis and a one-tailed hypothesis test to determine optimal performance among S3 and HDFS groups, which is then used to compare and assess benchmarks between the combinations of MapReduce settings and database schemas among the best-performing S3 and HDFS configuration. This bench will be illuminated across endless samples engulfed in the event horizon - a poorly managed lake - endless light, yet still, perilous darkness.




\section{Data}

This study investigates data warehouse models for housing stock price data and semi-structured alternative data. 
The stock price data was.
Twitter was chosen as the primary alternative data source because of ease-of-access the Twitter API and the large volume of available data.

\begin{table}
	\renewcommand{\arraystretch}{1.3}
	\caption{Data Features}
	\label{DataFeatures}
	\centering
	\begin{tabular}{c|l}
		\hline
		Source       & Features\\
		\hline
		\hline
		Stock Daily  & Prices: High, Low, Open, Close\\
		\hline
		\multirow{9}{*}{Stock Intraday} &  Prices: High, Low, Open, Close \\
		&  High Bollinger bands\\
		&  Mid Bollinger bands\\
		&  Low Bollinger bands\\ 
		&  Nominal moving average\\
		&  Historical moving average\\
		&  Signal moving average\\ 
		&  Exponential moving average\\
		&  Stochastic Slow K\\
		&  Stochastic Slow D\\
		\hline
		Tweets       & Text, URLs, Hashtags, Mentions, Users\\
		\hline
	\end{tabular}
\end{table}

\subsection{Stock Data}

The stock price data was collected through an API provided by Alpha Vantage.
This API provided access intraday stock prices, intraday price features, and daily prices.
Intraday stock data contained 35 features sampled at 15 minute intervals. 
The intraday features were from the following categories Bollinger bands, stochastic oscillators, moving averages, and exponential moving averages.
Approximately 1 millions rows of data were collected, which occurred from Oct. 04, 2019 to Oct. 24, 2019.
During the collection process, the data were recorded in files organized by day and category.
At the end of the collection process, the files for each category were combined and pushed to the HDFS datalake.


\subsection{Twitter Data}

sssss

\section{Data Warehouse Development}

This stock data is natively structured in a tabular form and naturally keyed by the combination of timestamp, stock symbol, and stock market.

\subsection{Normalized Schema}

%\begin{figure}
%	\centering
%	\includegraphics[width=2.5in]{NormalizedSchema.png}
%	\caption{Normalized Data Warehouse Schema}
%	\label{fig_sim}
%\end{figure}

Discuss normalizing the data - 3NF

\subsection{Optimized Schema}

Discuss the difference for optimization

\section{Technologies}

Rikel

\section{Results}



Maybe a table comparing read times for the different options

\begin{itemize}
	\item S3
	\item HDFS
	\item HDFS mapreduce settings?
\end{itemize}

\section{Analysis}

Analysis of the results


\section{Conclusion}

The conclusion goes here.

% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi

\begin{thebibliography}{1}

\bibitem{BuildingtheDWCH11}
W. H. Inmon, "Unstructured Data and the Data Warehouse," in 
  \emph{Building the Data Warehouse},
  4th ed. Hoboken: Wiley, 2005, ch. x, sec. x.
  Accessed on Nov. 6, 2019 [Online]. 
  Available: \\ https://learning.oreilly.com/library/view/building-the-data/9780764599446

\bibitem{WarehouseDesignApproaches}
I. Moalla, A. Nabli, L. Bouzguendam and M. Hammami,
 "Data warehouse design approaches from social media: review and comparison,"
 Social Network Analysis and Mining., Vol. 7, no. 1, pp. 1-14, Jan. 2017.
 Accessed on: Nov. 6, 2019 [Online]. Available doi: 10.1007/s13278-017-0423-8

\end{thebibliography}


% that's all folks
\end{document}


