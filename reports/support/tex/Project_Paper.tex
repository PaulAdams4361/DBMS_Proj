
%% bare_jrnl.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/

% formatting instructions
% https://ras.papercept.net/conferences/support/files/IEEEtran_HOWTO.pdf

% from here
%https://journals.ieeeauthorcenter.ieee.org/create-your-ieee-journal-article/authoring-tools-and-templates/ieee-article-templates/templates-for-transactions/

\documentclass[journal]{IEEEtran}

\usepackage{cite}
\usepackage{amsmath}
\usepackage{url}
\usepackage{multirow}
% don't use the borders around links
\usepackage[hidelinks]{hyperref}
\usepackage{listings}

% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
\graphicspath{{../images/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi

\begin{document}

\title{A Big Datalake Solution for\\ 
       Time Series Data}

\author{Paul~Adams, paula@smu.edu,
        Rikel~Djoko, rdjoko@smu.edu,
        and Stuart Miller, stuart@smu.edu}

% The paper headers
\markboth{MSDS7330 Term Paper}
{MSDS7330 Term Paper}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.

% make the title area
\maketitle

% This should be about 150-200 words
\begin{abstract}

In this paper we present a solution for warehousing time series of "Big Data"
 for structured data and semi-structured data.
Traditional systems, such as SQL databases, are well suited for storing highly structured data,
 but were not designed to store semi-structured forms of data.
Additionally, traditional systems, which operate with ACID transactions,
 can be low over large volumes of data.
"Big Data" systems were developed to store this type of data,
 addressing this gap in data storage system technology.
We developed a system to store structured and semi-structured time series data based on the 
 Apache Hadoop ecosystem.
We will also discuss the impact of schema design and horizontal scaling the Hadoop cluster.

\end{abstract}

\section{Introduction}

\IEEEPARstart{I}{n} recent times, the volume of data generation and
 data recording has been increasing.
The increase in data generation and recording is often referred to as "Big Data."
Traditionally, data generation systems were slow and the data was easily structured, 
 which lent to the use of traditional systems.
However, "Big Data" is often semi-structured or unstructured.

Every additional source of data that can act as a
medium for data communication may contain useful information, which,
with modern technology, can be structured and stored, accessible to any
who have the skills and need to make use of it \cite{BigDataComputing}. 


However, with the increasing ability to capture and store data from many
disparate sources, the need to store larger volumes of the data is
likewise an increasing issue when it comes time to access and use the data,
as many of the data gathered exist across very dynamic, diverse, and
large partitions. 
As such, the scalability of storing and accessing big data must increase
with it, relevant to the structures and locations of these data repositories. 
Developing a database with the Hadoop ecosystem, our team has built a data
warehousing solution to structure and store the data based on both
optimized and normalized schema designs, drafted from entity-relationship
models designed with the intention of enabling rapid storing and accessing
through the Hadoop MapReduce process \cite{BigDataComputing}. 
Through a combination of these systems and data storage within
Amazon Simple Storage Service (S3) and Apache Hadoop's native Hadoop
Distributed File System (HDFS), we analyze performance between two
approaches toward data- and task-parallelism using data gathered from
the stock market.
Motivated by the opportunity within "Big Data" to store and structure disparate data,
we designed a system to store stock data and Twitter data,
which could be used to service a large scale machine learning system.
To accomplish this, we aligned our project with a modern database
application called Hadoop. 
Hadoop provides a data lake ``ecosystem'' 
supporting both task- and data-parallelism across the many software
applications within the Apache suite in addition to software that can 
be managed and accessed within a network of servers (a compute cluster)
\cite{Intel, BigDataComputing}.

The server cluster enables users to read data from the same source,
simultaneously, as the data at that source is partitioned and
processed across multiple servers \textemdash a master and at least one slave. 
The processing framework that enables this is called MapReduce, 
which assigns nodes for ``mapping'' and ``reducing'' processes by applying  
various configurations, such as related to memory allocation and numbers
of mappers and reducers \cite{MappingReducing}. 
As data is mapped, it is reprocessed into a derivative data set,
split into tuples that are then processed across the cluster, in parallel,
and reassembled in the reduce process \cite{MappingReducing}. 
Following the reduce process, data is delivered to the end-user.

The parallelizability of Hadoop is central to this study as the primary
objective is to rapidly store and access financial market data for 
buy-sell decision-making. 
The scope of applied analysis in this study is focused on the ability to
scale and process a combination of quantitative stock market data and
qualitative Twitter data related to the quantitative data. 

Quantitative data was gathered from a stock market data vendor through an
Application Programming Interface (API) on 15-minute intervals using the R
programming language \cite{R}. 
Qualitative data was gathered and processed through a Twitter API
using the Python programming language. 
The data was structured and stored in a Hive data warehouse, 
which was created and managed using Hive Query Language (HQL).
R will be used via Open Database Connectivity (ODBC) to access and
build proof-of-concept models using the data. 
Once the data warehouse is developed,
Python will be deployed within the data lake to
derive predictive data through machine learning.

In order to provide manageable storage repositories that scale well to size and
data diversity, we have implemented this project using Simple Storage Service (S3)
and Elastic MapReduce (EMR) from Amazon Web Services (AWS). 
S3 and EMR are designed for large sets of data. 
Therefore, integrity of data and ingestion systems are to manage. 
This is essential in an environment that may need to support many simultaneous users,
each with different data needs. 
The EMR implementation in this project is housed across three 
- one master and two slave - Elastic Compute 2 (EC2) servers 
whereas S3 is a standalone repository within the AWS cloud suite.

Additionally, Apache Hive is used as a data warehouse 
because it is operated with HQL, 
which is easily communicable for SQL users. 
Furthermore, Hive allows for storage of massive databases tables and
is well-suited for big data application integration, 
including the Apache software suite. 
The data warehouse graphical user interface is provided by Cloudera Hue.

Two schemas were designed for the warehouse in a star configuration.
This first schema was designed in a full normalized fashion,
which is known as a snowflake schema.
The second schema was based on the snowflake schema, 
but denormalized to limit the number of tables, 
which limits the number of joins required in queries.
Query performance will be measured on these schemas to determine
which design will be better for this use case. 

Performance of the data warehouse is based on time taken to process the data
collected for this project into the Hive data warehouse,
which includes table creation and loading. 
We will use a repeated measures analysis and a one-tailed hypothesis 
test to determine optimal performance among S3 and HDFS groups, 
which is then used to compare and assess benchmarks between the 
combinations of MapReduce settings and database schemas among the
best-performing S3 and HDFS configuration.

\section{Data}

We collected stock data and Twitter data for this study.
Both daily and intraday stock price data were collected
 for use in this analysis. 
Twitter\footnote{https://www.twitter.com/}
 was chosen as the primary source of alternative data because of
 ease-of-access to the 
 \href{https://developer.twitter.com/en/docs}{Twitter API}\footnote{https://developer.twitter.com/en/docs}
 and the large volume of available data.
Categories of data are summarized in Table \ref{DataCategories} and a full
 list of features is given in Appendix \ref{DataFeatures}.

\begin{table}
    \renewcommand{\arraystretch}{1.3}
    \caption{Data Categories}
    \label{DataCategories}
    \centering
    \begin{tabular}{c l}
        \hline
        \hline
        \textbf{Source}       & \textbf{Features}\\
        \hline
        \multirow{3}{*}{Stock Daily} &  Symbol \\
        &  Time Stamp \\
        &  Prices \\
        \hline
        \multirow{6}{*}{Stock Intraday} &  Symbol \\
        &  Time Stamp \\
        &  Prices \\
        &  Bollinger Bands \\
        &  Moving Averages \\
        &  Stochastic Indicators \\
        \hline
        \multirow{7}{*}{Twitter} &  Symbol \\
        &  Time Stamp \\
        &  User \\
        &  Text \\
        &  URLs \\
        &  Hashtags \\
        &  Mentions \\
        \hline
        \hline
    \end{tabular}
\end{table}

\subsection{Stock Data}

The stock price data was collected through an API provided by
 \href{https://www.alphavantage.co/}{Alpha Vantage}\footnote{https://www.alphavantage.co/}.
The R\footnote{Version 3.6.0}
 programming language was used to collect data from the Alpha Vantage API
\cite{R}.
This API provided access to daily prices, intraday stock prices,
 and intraday price features from the following categories Bollinger bands, 
 stochastic oscillators, moving averages, and exponential moving averages. 

The stock data was collected from Oct. 04, 2019 to Oct. 24, 2019 on
 all 2520 symbols traded on the NYSE and all 3213 symbols traded on the NASDAQ.
A single value was stored for each feature of the daily prices.
The intraday values were sampled at 15 minute intervals from market open
 to market close.
This resulted in a total of 49, 093, 917 stock data points.


\subsection{Twitter Data}

Messages on Twitter (called tweets) are mainly comprised of
 tweet ID, timestamp, user (screen name), and text.
Tweets can also contain many other features such as
 URLs, hashtags, emojis,  and mentions.
For this analysis, in addition to the main features,
 URLs, hashtags, and mentions were also collected in the data warehouse.
A mention is a reference to another Twitter user's screen name
 in the text of a tweet.
A hashtag is some collection of characters starting with
 '\#' without white space.
Generally, the character portion of a hashtag will be a word or set of words,
 but this is not necessary.
An example of a tweet containing one hashtag (\texttt{\#FF}) and one 
mention (\texttt{@everett}) is given in Listing \ref{SampleTweet} (the value of
 '\texttt{full\_text}').

We collected 423, 802 tweets from 108 Twitter users, using the
 python\footnote{Version 3.6.8}
 module Twython\footnote{https://pypi.org/project/twython/3.6.0/}.
The data from Twitter is returned in JavaScript Object Notation (JSON); 
 an example is shown in Listing \ref{SampleTweet}.
The features of interest were extracted from the JSON files and reformatted in tab
 separated files (TSV) using python and the built-in JSON and CSV modules.
After the tweets were extracted from Twitter,
 mentions of company names or stock symbols were extracted from the tweet text by
 matching sections of tweet text to company names and company stock symbols.
All other featured were extracted from the JSON by key.

As shown in Listing \ref{SampleTweet}, the timestamp, text, and tweet ID
 can be accessed at the top level of the JSON object with \texttt{created\_at},
 \texttt{full\_text}, and \texttt{id\_str}, respectively. 
The user information can be accessed from the key \texttt{user}, 
 which accesses another set of objects. 
The values for \texttt{screen\_name} and \texttt{id\_str} were extracted for the user.
The hashtags, mentions, and URLs are located under the key \texttt{entities} as
 \texttt{hashtags}, \texttt{user\_mentions}, and \texttt{urls}, respectively.
Each of these keys under \texttt{entities}, return an array of objects,
 containing the features of interest. 
The value for \texttt{text} under \texttt{hashtags} was the only value collected
 for the hashtags (\texttt{\#FF} in the example).
The values for \texttt{screen\_name} and \texttt{id\_str} under 
 \texttt{user\_mentions} were collected for mentions (\texttt{even\_everett} and
 \texttt{21841004}, respectively, in the example).
Only the value of \texttt{expanded\_url} was collected user \texttt{urls}
 (\texttt{http://www.you.com/watch?v=TZR} in the example).

\lstset{
	breaklines=true,
	emph={
		created\_at,
	    full\_text,
	    id\_str,
	    entities,
	    hashtags,
	    urls,
	    user\_mentions,
	    user,
	    expanded\_url,
	    text,
	    screen\_name
      },
	emphstyle=\textbf
}

% example tweet json; this is a fake
\begin{lstlisting}[frame=single,
 caption={Sample Tweet JSON},
 label={SampleTweet}]
{'created_at': 'Fri Oct 25 07:44:38 +0000 2019',
 'id': 1187636067505188864,
 'id_str': '1187636067505188864',
 'full_text': '@everett where do I start with this company? #FF!!!!',
 'truncated': False,
 'display_text_range': [0, 52],
 'entities': {
   'hashtags': [
            {'text': 'FF',
             'indices': [45, 48]}],
   'symbols': [],
   'user_mentions': [
          {'screen_name': 'even_everett',
           'name': 'Even Everett',
           'id': 21841004,
           'id_str': '21841004',
           'indices': [0, 10]}],
   'urls': [
     "indices": [32, 52],
     "url": "http://t.co/TZR",
     "display_url": "you.com/watch?v=oHg…",
     "expanded_url": "http://www.you.com/watch?v=TZR"]}
 'user': {'id': 14216123,
   'id_str': '14216123',
   'name': 'Jim Jim',
   'screen_name': 'jimjim',
   ...}
}
\end{lstlisting}

\begin{figure}
	\centering
	\includegraphics[width=2.5in]{SnowFlake_Schema_Simple.png}
	\caption{Conceptual Diagram of the Data Warehouse Snowflake Schema}
	\label{snowflake}
\end{figure}

\section{Data Warehouse Development}
\label{DataWarehouseDevelopment}

Data warehouses are often designed with a \textit{star} schema 
 \cite{BuildingtheDWCH11}.
In a star schema design, there is a central table (called the fact table),
 which contains the unifying features of the dataset and keys to other tables
 \cite{BuildingtheDWCH11}.
The tables surrounding the fact table (called dimension tables) 
 contain information related to a category of the facts \cite{Enterprise}.
In the dataset for this analysis, the unifying features are timestamp and
 company stock symbol.
The combination of timestamp and symbol is a natural key into the stock data and
 the twitter data.
These features were included on the fact table as the primary key.
The company name and exchange market associated with the symbols were 
 included on the fact table for convenience.
The natural dimensions of the fact table are intraday stock data,
 daily stock data, and tweet data.

In some cases, dimensions can be normalized into multiple tables. 
When the dimensions of a star schema are are normalized, the schema
 is in its \textit{snowflake} form \cite{WarehouseDesignApproaches}.
In this study, two schemas were designed: one in snowflake form and
 the other in a denormalized form.
We will consider a star schema in snowflake form when it is normalized into
 third normal form (3NF), meaning the domains of the attributes are atomic and
 there are no functional dependencies on a portion of the primary key \cite{DSCNormalization}.


\begin{figure}
	\centering
	\includegraphics[width=2.5in]{Star_Schema_Simple.png}
	\caption{Conceptual Diagram of the Data Warehouse 
		Denormalized Star Schema}
	\label{star}
\end{figure}

\subsection{Snowflake Schema}

As noted previously, the fact table of this star design contained the
 company symbols and timestamps as a composite primary key.
The three dimensions of fact table, daily stock data, intraday stock data, and 
 tweet data, share the primary key of the fact table.

The stock data comes in a form suitable for the snowflake design; 
 it is already in 3NF.
However, the intraday data was split into five tables for a more 
 general use schema design: prices, Bollinger bands, moving averages,
 and stochastic indicators.
The daily stock table and the intraday stock tables are shown joining to the 
 fact table in Fig. \ref{snowflake} (left side of the schema diagram).

Like the stock data, the tweet data came with a timestamp, but stock symbol
 is not a nominal feature of tweets.
The stock symbol feature was generated by extracting matching strings
 during the data collection process; therefore, stock symbols are not
 guaranteed to be non-null in the tweet dimension.
Thus, the same features can still be used to join the tweet dimension to the 
fact table, but cannot serve as a primary key.
However, Twitter assigns a tweet ID for each tweet, which is guaranteed to be
 a primary key.
The tweet ID was used as the primary key of the tweet table.

As noted previously, three secondary features of tweets are used in this study.
Each of these features, mentions, hashtags, and URLs, is a normalizable
 feature of the tweet dimension.
To achieve a 3NF design, the unique attributes of the tweets were collected
 in a central table and tables were created for mentions, hashtags, and URLs.
Since there is a many-to-many cardinality between the central tweet table and each of 
 the three secondary members, join tables were created to connect the secondary tables 
 to the central tweet table. 
The result of the tweet dimension normalization is shown conceptually
 in Fig. \ref{snowflake} (right side of schema diagram).

\subsection{Denormalized Star Schema}

While the snowflake schema is suitable for general use cases.
The number of tables should be limited to decrease query read time to support
 fast data transfer to a machine learning system.
The number of intraday tables and tweet tables were reduced to support
 fast query times.
All intraday tables were combined into one table. 
Each secondary member of the normalized tweet dimension was
 subsumed into a copy of the main tweet table, reducing the number of tables
 in the tweet dimension from seven to three.
The schema resulting from this denormalization process is shown
 conceptually in Fig. \ref{star}.


\section{Implementation}

Multiple vendors provide platforms and infrastructure for data warehousing technologies, 
 this project was implemented using Amazon Web Services (AWS).
AWS is a set of cloud computing services provided by Amazon.com\footnote{https://aws.amazon.com/}
 that are accessible over the internet.
AWS provides multiple services for many different applications. 
Elastic MapReduce (EMR) was used for the implementation of Hadoop in this project.

\subsection{AWS Elastic MapReduce}

AWS EMR is a pre-configured compute cluster for Big Data.
The cluster can be provisioned and terminated on demand as needed.
It comes with a configurable set of the Hadoop ecosystem elements pre-installed and ready to use.
The EMR cluster used in this study was provisioned with three
 m5.xlarge\footnote{https://aws.amazon.com/ec2/instance-types/m5/}
 elastic compute instances using version 5.27.0 of the EMR 
 software\footnote{emr-5.27.0 contains Amazon Hadoop 2.8.5, Hive 2.3.5, and Hue 4.4.0. See https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-release-5x.html}.

\subsection{Apache Hadoop}

Hadoop is an open source software framework for the distributed storage and
 distributed processing of a very large datasets. 
The core of Apache Hadoop consists of a storage part: Hadoop Distributed File System
 (HDFS) and a processing part MapReduce.

\subsubsection{Hadoop Distributed File System}

Unlike traditional file system, HDFS is a distributed file system designed to run on commodity hardware. 
HDFS the architecture consists of a master (NameNode) and at least one slave (DataNodes)\cite{HDFSarchitecture}.
The NameNode is the controller which consist storing data and metadata of the data no the DataNodes.
 The metadata includes a Namespace lookup table used to locate each file from the DataNodes. 
The Fig. \ref{HDFS} shows the Architecture of HDFS \cite{HDFS}.

\subsubsection{Hadoop MapReduce}

Hadoop MapReduce is a programming model for large scale data processing. 
With the high demand of computing power,
 computer architecture has evolved over the years from serial computing to parallel computing
 where tasks are distributed and executed simultaneously across multiple processors within the same computer. 
With MapReduce,
 instead of using one server instance with multiple processors, 
 multiple servers with multiple processors are used for computation.
This is called distributed computing system. 
The parallelism in Hadoop is categorized into data-parallelism and task-parallelism.
Data-parallelism is focused on processing data across multiple processors whereas
 task-parallelism is focused on distributing execution threads across multiple servers \cite{Parallelism}. 
Utilizing one or the other is dependent upon whether data is being provided into
 the system or being utilized for events such as machine learning. 
This is what enables management and processing of "Big Data" (multi-terabyte datasets)
 in parallel on large clusters (thousands of nodes) of commodity hardware in a reliable,
 fault-tolerant manner. 
A MapReduce framework consists of two types of workers: mappers and reducers.

MapReduce uses the divide and conquer technique where the input is divided into a set
 of small tasks and each task is identified by a key-value pair \cite{Divide-and-Conquer}.
The key serves as a task ID and the value as the task output.
Each task is then processed and executed by the mapper and
the outputs are processed and merged by the reducer \cite{MapReduce}.

\subsection{AWS Simple Storage Service}

As the name implies, the S3 is a storage system.
It is used to store and retrieve any amount of data any time, from anywhere on the web. 
S3 is reliable, fast, and inexpensive.

\subsection{Apache Hive}

Apache Hive is a data warehouse application built on top of Hadoop. We used Hive to structure, organize, model, and query our data using the Hive Query Language, HQL. We structured files from within the Hadoop Distributed File System (HDFS) and loaded them into tables within Hive. Hive operates by using the HDFS storage to generate tables. However, when Hive queries are executed, the data is retrieved from HDFS via the compiler, which uses an execution engine and metastore to return results. See: http://www.guardxinc.com/blog/2017/04/10/hive-queries-whats-really-going-on-in-there/

\subsection{Cloudera Hue}

Cloudera Hue is an open source web application that served as user interface for Hadoop components.
Hue provides access to Hadoop from within a browser, allowing users to interact with the Hadoop ecosystem applications. 
Hue is an alternative to accessing the Hadoop ecosystem applications from the command line interface.
Hue was used to interact with the EMR cluster and run Hive scripts.

\begin{figure}
    \centering
    \includegraphics[width=2.5in]{HDFS_Arch.png}
    \caption{HDFS Architecture \cite{HDFS}}
    \label{HDFS}
\end{figure}

\section{Study Design}
\label{StudyDesign}

In this study, we will investigate the impact of schema normalization and
 EMR cluster size on the performance of a read query.
Both schemas presented in Section \ref{DataWarehouseDevelopment} were tested on
 EMR clusters with 3 nodes and 5 nodes.
This is a two-way Analysis of Variance (ANOVA) where ERM cluster size and schema
 are the explanatory factor variables and the response variable is query
 execution time.


[Paul update]
During performance testing, we gathered all fields from all tables, joining all tables in the schema together. Joining all tables enabled capturing the entirety of the database, under each schema, to produce the same results. These queries were limited to 75,000 returned records to enable gathering more runtime samples. This number was decided to be reasonable based on the difference in performance time. As noted in the analysis section below, the performance time between queries was significant at 75,000 records; limited a higher volume of returned records would have increased the significance. Therefore, 75,000 was determined reasonable for analysis.

\begin{table}
	\renewcommand{\arraystretch}{1.3}
	\caption{Results Table}
	\label{ResultsTable}
	\centering
	\begin{tabular}{c c c c}
		\hline
		\hline
		\multirow{2}{*}{\textbf{Schema}} & \multirow{2}{*}{\textbf{Cluster Nodes}} &
		\multicolumn{2}{c}{\textbf{Query Time} (seconds)}   \\
		\cline{3-4}
		&     & Mean & Standard Deviation\\
		\hline
		\multirow{2}{*}{Snowflake}         & 3   & 121.05 & 4.12 \\
		
		& 5   & 94.49 & 4.01  \\
		
		\hline
		\multirow{2}{*}{Denormalized Star} & 3   & 103.77 & 2.99 \\
		
		& 5   & 79.68 & 5.86  \\
		
		\hline
		\hline
	\end{tabular}
\end{table}

\section{Results}

As discussed in Section \ref{StudyDesign}, a two-way ANOVA test was performed.
We identified that the interaction between schema design and server count provide significant results,
indicating that there is a difference in the proportions of performance speed between the two schemas when considering count of servers in a cluster.
The dependence of performance on combinations of schema and cluster size is apparent in Fig. \ref{Results},
 which shows boxplots of the query times of each combination of schema and cluster size.
The red boxes represent the query times for the snowflake schema and
 the blue boxes represent the query times for the denormalized schema.
The boxplot is faceted by cluster size with the three-node results on the right and
 the five-node results on the left.
The query time of the denormalized schema is lower than the normalized schema for both sizes of clusters.

We find that, while holding cluster size constant,
 schema design is significant on performance across both server sizes
 (p-value $< 0.0001$, F $ = 999.187$). 
Denormalizing the snowflake schema provided a 14.27\% decrease in mean query time on
 the three-node cluster and a 15.67\% decrease mean in query time on the five-node cluster.
Furthermore, while holding schema design constant, 
 performance speed difference was also significant 
 (p-value $< 0.0001$, F $= 3399.954$) 
 between the two cluster sizes with the five-node cluster outperforming
 the three-node cluster. 
A summary of the results is shown in Table \ref{ResultsTable}.


\section{Analysis}

In analyzing the exploratory graphical analysis of the clusters, 
 the interaction between schema and cluster size is such that as cluster size increases,
 the normalized schema approaches a similar level of performance. 
However, testing indicates there is still a statistically significant difference between 
 performance at the five-server cluster level. 
Further testing could identify that, while holding database size constant, at some server level, 
 the normalized schema may outperform the de-normalized schema. 
However, this is based on linear approximation that may become non-linear with more samples and clusters. 
Regardless, under both cluster sizes,
 the de-normalized schema outperformed the normalized schema by a level of statistical significance.

\section{Conclusions}

\textbf{EXAMPLE Conclusion:}
The scalability of large sets of data is of ever-increasing importance in the data community, which itself is ever-increasing with the advancement of modern technology. Modern technology itself is enabling both an increase in both data volume as well as data diversity being captured and stored. Consequently, parallelism is of utmost importance in making use of this data. Herein analyzed are methods that are proven within the scope of big data. As modern technology continues to advance, these parallel principals will serve as the root from which methodologies will be developed. As with the businesses and industries the data captured will serve, there will remain a trade-off that must be assessed for each business model. This study has provided two primary storage methods - S3 and HDFS - in addition to different data warehousing schema design - normalized versus optimized, star versus snowflake - and data distribution - herein, MapReduce - techniques. Through analysis using repeated measures, S3 outperforms HDFS when X, Y, Z and HDFS outperforms S3 when Z, Y, X. Furthermore, MapReduce configurations are constant across both storage methods - HDFS and S3 - with an optimized star schema performing faster, but a normalized snowflake schema performing more reliably within a conventional three-tiered system architecture.

% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi





\begin{figure}
	\centering
	\includegraphics[width=2.5in]{Rplot.png}
	\caption{Average of Query Time by Schema and Block Size with Standard Deviation Bars}
	\label{Results}
\end{figure}

\appendices

\section{Data Features}
\label{DataFeatures}

This section contains a full listing of all the features collected for this
 study. The features are listed by general category as shown in 
 Table \ref{DataCategories}.

\subsection{Intraday Stock Features}

The intraday stock features are described below and are organized in the 
 following categories: Key, Prices, Bollenger Bands, Moving Averages, and Stochastic 
 Indicators.

\subsubsection{Key}

\begin{itemize}
	\item Symbol
	\item Time
	\item Date
\end{itemize}

\subsubsection{Prices}

\begin{itemize}
	\item Open
	\item Close
	\item High
	\item Low
\end{itemize}

\subsubsection{Bollenger Bands}

\begin{itemize}
	\item Open
	\begin{itemize}
		\item Lower Band
		\item Middle Band
		\item Upper Band
	\end{itemize}
	\item Close
	\begin{itemize}
		\item Lower Band
		\item Middle Band
		\item Upper Band
	\end{itemize}
	\item High
	\begin{itemize}
		\item Lower Band
		\item Middle Band
		\item Upper Band
	\end{itemize}
	\item Low
	\begin{itemize}
		\item Lower Band
		\item Middle Band
		\item Upper Band
	\end{itemize}
\end{itemize}

\subsubsection{Moving Averages}

\begin{itemize}
	\item Open
	\begin{itemize}
		\item Convergence Divergence
		\item Convergence Divergence Signal
		\item Convergence Divergence Historical
		\item Exponential
	\end{itemize}
	\item Close
	\begin{itemize}
		\item Convergence Divergence
		\item Convergence Divergence Signal
		\item Convergence Divergence Historical
		\item Exponential
	\end{itemize}
	\item High
	\begin{itemize}
		\item Convergence Divergence
		\item Convergence Divergence Signal
		\item Convergence Divergence Historical
		\item Exponential
	\end{itemize}
	\item Low
	\begin{itemize}
		\item Convergence Divergence
		\item Convergence Divergence Signal
		\item Convergence Divergence Historical
		\item Exponential
	\end{itemize}
\end{itemize}

\subsubsection{Stochastic Indicators}

\begin{itemize}
	\item 5-day indicator
	\item 3-day indicator
\end{itemize}

\begin{thebibliography}{1}

\bibitem{BigDataComputing}
R. Kune, P. Konugurthi, A. Agarwal, R. Chillarige, R. Buyya,
 "The Anatomy of Big Data Computing," Software: Practice and Experience,
 Vol. 46 no. 1, pp.79-105, Jan. 2016. 

\bibitem{BuildingtheDWCH11}
W. H. Inmon, "Unstructured Data and the Data Warehouse," in 
  \emph{Building the Data Warehouse},
  4th ed. Hoboken: Wiley, 2005, ch. 11.
  Accessed on Nov. 6, 2019 [Online]. 
  Available: \\ https://learning.oreilly.com/library/view/building-the-data/9780764599446

\bibitem{WarehouseDesignApproaches}
I. Moalla, A. Nabli, L. Bouzguendam and M. Hammami,
 "Data warehouse design approaches from social media: review and comparison,"
 Social Network Analysis and Mining., Vol. 7, no. 1, pp. 1-14, Jan. 2017.
 Accessed on: Nov. 6, 2019 [Online]. 
 Available doi: 10.1007/s13278-017-0423-8

\bibitem{Enterprise}
A. Gorelik, "Historical Perspectives," in 
 \emph{The Enterprise Big Data Lake},
 1st ed. Sebastopol, CA: Wiley, 2019, ch. 2, pp. 25-47.

\bibitem{Intel}
 "Extract, Transform, and Load Big Data with Apache Hadoop," Intel, USA, 2013.
 Available:\\ https://software.intel.com/sites/default/files/article/402274/etl-big-data-with-hadoop.pdf

\bibitem{HDFS}
D. Borthakur, HDFS Architecture Guide, The Apache Software Foundation,
 August 22 2019. Accessed on: Nov. 8, 2019. [Online] Available: \\
 https://hadoop.apache.org/docs/r1.2.1/hdfs\_design.html

\bibitem{Parallelism}
D. Sitaram, G. Manjunath \textit{Moving To The Cloud}. Boston: Syngress, 2012, pp. 205–253.

\bibitem{Divide-and-Conquer} 
S. Zhao, R. Li, W. Tian, W. Xiao, X. Dong, D. Liao, S. U. Khan, L. Keqin, 
 “Divide-and-conquer approach for solving singular value decomposition based on MapReduce,” Abbrev. Title of Journal, vol. 28, no. 2, pp. 331–350, Nov. 2016. 
 Accessed on: Nov. 10, 2019 [Online]. Available: doi:10.1002/cpe.3436, 

\bibitem{MapReduce}
D. Miner, A. Shook, MapReduce Design Patterns, 1st ed. Sebastopol, CA: O'Reilly Media, 2012,
 pp. 5–6.

\bibitem{MappingReducing}
D. Miner, A. Shook, Hadoop MapReduce v2 Cookbook, 2nd ed. Sebastopol, CA: O'Reilly Media,
 2015, pp. 1.
 
\bibitem{HDFSarchitecture}
T. White, Hadoop: The Definitive Guide, 1st ed. Sebastopol,
 CA: O'Reilly Media, 2009, pp. 44.

\bibitem{R}
R Core Team, R: A language and environment for Statistical Computing, Vienna, Austria: 2018.
 Available: https://www.R-project.org/
 
\bibitem{Sleuth}
F. Ramsey and D. Schafer, "Repeated Measures and Other Multivariate Responses," 
 in \textit{The Statistical Sleuth}, 3rd ed. Boston: Brooks/Cole, 2013,
 ch. 16, pp. 479-485.  

\bibitem{RepeatMeasures}
M. Crawley, "Analysis of Variance," in \textit{The R Book},
 1st ed. Chichester, West Sussex, United Kingdom: Wiley, 2013,
 ch. 11, pp. 470-478.

\bibitem{OptimizeHadoop}
K. Tannir, "Enhancing Map and Reduce Tasks," in 
 \textit{Optimizing Hadoop for MapReduce},
 1st ed. Birmingham, England: Packt Publishing Ltd, 2014,
 ch. 5. Accessed on: Nov. 20, 2019 [Online]. 
 
\bibitem{DSCNormalization}
A. Silberschatz, H. Korth, and S. Sudarshan, "Relational Database Design," in
 \textit{Database Design Concepts}, 6th ed. NY: McGraw-Hill, 2011, ch. 8, sec. 8.3,
 pp. 329 - 338. 

\end{thebibliography}



% that's all folks
\end{document}


